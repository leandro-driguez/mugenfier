{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oCF3wKMDNp6"
      },
      "source": [
        "# Music Genre Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XfiDBNMDNp_",
        "outputId": "b290ca4e-3dd2-41f1-a731-b22794885019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install wget\n",
        "import os\n",
        "import cv2\n",
        "import wget\n",
        "import random\n",
        "import shutil\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from pandas import DataFrame\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Input, Cropping2D, Flatten, Dense, Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TrKBv-RiDNqB"
      },
      "outputs": [],
      "source": [
        "SEED_VALUE = 42\n",
        "\n",
        "# Fix seed to make training deterministic.\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "tf.random.set_seed(SEED_VALUE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq-JEy-vDNqB"
      },
      "source": [
        "## Load the GTZAN Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ4Ip6g-DNqC",
        "outputId": "593a44c2-bf41-4567-ff69-8bc58058739f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if os.getenv('COLAB_RELEASE_TAG'):\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TurG6yZ1DNqC"
      },
      "outputs": [],
      "source": [
        "PREPROCESSING = False\n",
        "\n",
        "try:\n",
        "    if 'dataset' not in os.listdir('/content'):\n",
        "        os.mkdir('/content/dataset/')\n",
        "    pwd = os.getcwd()\n",
        "    os.chdir('/content/dataset/')\n",
        "\n",
        "    if 'preprocessing.tar.gz' not in os.listdir('.'):\n",
        "        if os.getenv('COLAB_RELEASE_TAG'):\n",
        "            if 'preprocessing.tar.gz' in os.listdir('/content/gdrive/MyDrive'):\n",
        "                shutil.copy2('/content/gdrive/MyDrive/preprocessing.tar.gz', '.')\n",
        "            else:\n",
        "                # make sure to download the GTZAN dataset from \"https://drive.google.com/file/d/1UdmqcrBw71EgOtCLy6ic_C6EDBz9KQt_/view?usp=share_link\"\n",
        "                # upload it to your own google drive for it to be copied in the previous if statement block\n",
        "                pass\n",
        "        else:\n",
        "            if pwd != '/content/dataset':\n",
        "                if 'preprocessing.tar.gz' in os.listdir(f'{pwd}/dataset'):\n",
        "                    shutil.copy2(f'{pwd}/dataset/preprocessing.tar.gz', '.')\n",
        "                else:\n",
        "                    raise Exception(\"Download the GTZAN dataset preprocessed.\")\n",
        "\n",
        "        tar = tarfile.open('preprocessing.tar.gz', 'r:gz')\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "\n",
        "finally:\n",
        "    GENRES = os.listdir('/content/dataset/preprocessing/mfcc')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jht-abLDNqD"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VDRogc3lDNqD"
      },
      "outputs": [],
      "source": [
        "def load_data(src: str, feature: str, \n",
        "              random_state: float = SEED_VALUE, shuffle: bool = True, \n",
        "              stratify: list = None):\n",
        "    dataset = []\n",
        "    for genre in os.listdir(f'{src}/{feature}'):\n",
        "        for img in os.listdir(f'{src}/{feature}/{genre}'):\n",
        "            img = cv2.imread(f'{src}/{feature}/{genre}/{img}')\n",
        "            # img = cv2.resize(img, (256, 192))\n",
        "            img = np.array(img, dtype=np.float32)\n",
        "            dataset.append([img, genre])\n",
        "\n",
        "    df = DataFrame(data=np.array(dataset, dtype=object), columns=[feature, 'genre'])\n",
        "\n",
        "    one_hot = pd.get_dummies(df['genre'])\n",
        "\n",
        "    df = pd.concat([df, one_hot], axis=1)\n",
        "    df.drop(['genre'], axis=1, inplace=True)\n",
        "    \n",
        "    strat = df[stratify] if stratify else None\n",
        "    train_set, test_set = train_test_split(df, test_size=0.5, random_state=random_state, \n",
        "                     shuffle=shuffle, stratify=strat)\n",
        "\n",
        "    return (np.array([tf.convert_to_tensor(img) for img in train_set['mfcc']]), train_set[GENRES])\\\n",
        "          , (np.array([tf.convert_to_tensor(img) for img in train_set['mfcc']]), test_set[GENRES])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gfOsnG1BDNqE"
      },
      "outputs": [],
      "source": [
        "(X_train, _), (X_test, _) = load_data('./preprocessing/', 'mfcc', stratify=GENRES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXuwBf8uDNqE"
      },
      "source": [
        "## CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nt2KtEizDNqF"
      },
      "outputs": [],
      "source": [
        "def generate_autoencoder():\n",
        "    input_l=Input(shape=(217,334,3))\n",
        "\n",
        "    maxp_ini=MaxPooling2D((4,6), padding='same')(input_l)\n",
        "    encoding_1=Conv2D(12, (3,3), activation='relu',padding='same')(maxp_ini)\n",
        "\n",
        "    maxp_1=MaxPooling2D((2,2), padding='same')(encoding_1)\n",
        "    encoding_2=Conv2D(6, (3,3), activation='relu',padding='same')(maxp_1)\n",
        "\n",
        "    maxp_2=MaxPooling2D((2,2), padding='same')(encoding_2)\n",
        "    encoding_3=Conv2D(3, (3,3), activation='relu',padding='same')(maxp_2)\n",
        "\n",
        "    bottleneck=MaxPooling2D((2,2), padding='same')(encoding_3)\n",
        "\n",
        "    decoding_2=Conv2D(3, (3,3), activation='relu', padding='same')(bottleneck)\n",
        "    Up_2=UpSampling2D((2,2))(decoding_2)\n",
        "\n",
        "    decoding_3=Conv2D(6, (3,3), activation='relu', padding='same')(Up_2)\n",
        "    Up_3=UpSampling2D((2,2))(decoding_3)\n",
        "\n",
        "    decoding_4=Conv2D(12, (3,3), activation='relu', padding='same')(Up_3)\n",
        "    Up_4=UpSampling2D((2,2))(decoding_4)\n",
        "    crop_1=Cropping2D(((0,1), (0,0)))(Up_4)\n",
        "\n",
        "    decoding_5= Conv2D(3,(3,3),activation='sigmoid',padding='same')(crop_1)\n",
        "    up_out=UpSampling2D((4,6))(decoding_5)\n",
        "    output_l=Cropping2D(((0,3), (0,2)))(up_out)\n",
        "\n",
        "    autoencoder=Model(inputs=[input_l],outputs=[output_l])\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBeIyu3iDNqF",
        "outputId": "825a4620-d5b5-4bb4-9fb2-8009c475a2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 217, 334, 3)]     0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 55, 56, 3)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 55, 56, 12)        336       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 28, 28, 12)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 6)         654       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 3)         165       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 3)          0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 3)           84        \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 14, 14, 3)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 6)         168       \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 28, 28, 6)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 12)        660       \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 56, 56, 12)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " cropping2d (Cropping2D)     (None, 55, 56, 12)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 55, 56, 3)         327       \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 220, 336, 3)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " cropping2d_1 (Cropping2D)   (None, 217, 334, 3)       0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,394\n",
            "Trainable params: 2,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generate_autoencoder().summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((X_train, X_test), axis=0)\n",
        "X_train = X_train / 255"
      ],
      "metadata": {
        "id": "gr4CfUOO7xgb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhu2oqEMDNqF",
        "outputId": "1ebd2683-74e1-40c9-ac98-3313118999cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 24s 663ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 19s 603ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 21s 648ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 19s 570ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 20s 631ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
            "Epoch 6/10\n",
            "23/32 [====================>.........] - ETA: 5s - loss: 0.0037 - mean_squared_error: 0.0037"
          ]
        }
      ],
      "source": [
        "history = []\n",
        "\n",
        "model_storage = '/content'\n",
        "\n",
        "if os.getenv('COLAB_RELEASE_TAG'):\n",
        "    model_storage = '/content/gdrive/MyDrive'\n",
        "\n",
        "if 'model_storage' not in os.listdir(model_storage):\n",
        "    os.mkdir(f'{model_storage}/model_storage')\n",
        "\n",
        "autoencoder = generate_autoencoder()\n",
        "last_iter = 0\n",
        "\n",
        "if len(os.listdir(f'{model_storage}/model_storage')) > 0:\n",
        "    last_iter = sorted([int(autoencoder.removeprefix(f'autoencoder_model_').removesuffix('.keras'))\n",
        "        for autoencoder in os.listdir(f'{model_storage}/model_storage')], reverse=True)[0]\n",
        "    autoencoder = tf.keras.saving.load_model(f'{model_storage}/model_storage/autoencoder_model_{last_iter}.keras')\n",
        "\n",
        "for i in range(last_iter, 50):\n",
        "    history.append(autoencoder.fit(x=X_train, y=X_train, epochs=10))\n",
        "    autoencoder.save(f'{model_storage}/model_storage/autoencoder_model_{i}.keras')\n",
        "\n",
        "kfold=KFold(n_splits=10, shuffle=True)\n",
        "fold_no=1\n",
        "final_errors_per_fold=[]\n",
        "\n",
        "for train, _ in kfold.split(X_train, X_train):\n",
        "    last_iter = 0\n",
        "\n",
        "    autoencoder = generate_autoencoder()\n",
        "\n",
        "    if len(os.listdir(f'{model_storage}/model_storage')) > 0:\n",
        "        last_iter = sorted([int(autoencoder.removeprefix(f'autoencoder_model_{fold_no}_').removesuffix('.keras'))\n",
        "            for autoencoder in os.listdir(f'{model_storage}/model_storage')], reverse=True)[0]\n",
        "        autoencoder = tf.keras.saving.load_model(f'{model_storage}/model_storage/autoencoder_model_{fold_no}_{last_iter}.keras')\n",
        "\n",
        "    for i in range(last_iter, 50):\n",
        "        history.append(autoencoder.fit(x=X_train[train], y=X_train[train], epochs=10))\n",
        "        autoencoder.save(f'{model_storage}/model_storage/autoencoder_model_{i}.keras')\n",
        "    \n",
        "    # Generate generalization metrics\n",
        "    scores = autoencoder.evaluate(X_train, X_train, verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {autoencoder.metrics_names[0]} of {scores[0]}')\n",
        "    final_errors_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}