{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6oCF3wKMDNp6"
      },
      "source": [
        "# Music Genre Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XfiDBNMDNp_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from pandas import DataFrame\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Input, Cropping2D, Cropping3D, Flatten, Dense, Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrKBv-RiDNqB"
      },
      "outputs": [],
      "source": [
        "SEED_VALUE = 42\n",
        "\n",
        "# Fix seed to make training deterministic.\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "tf.random.set_seed(SEED_VALUE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq-JEy-vDNqB"
      },
      "source": [
        "## Load the GTZAN Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ4Ip6g-DNqC",
        "outputId": "86598be3-da45-4ef2-9a44-b5ddbd8d6105"
      },
      "outputs": [],
      "source": [
        "if os.getenv('COLAB_RELEASE_TAG'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TurG6yZ1DNqC"
      },
      "outputs": [],
      "source": [
        "PREPROCESSING = False\n",
        "\n",
        "try:\n",
        "    if 'dataset' not in os.listdir('/content'):\n",
        "        os.mkdir('/content/dataset/')\n",
        "    pwd = os.getcwd()\n",
        "    os.chdir('/content/dataset/')\n",
        "\n",
        "    if 'gtzan-features.zip' not in os.listdir('.'):\n",
        "        if os.getenv('COLAB_RELEASE_TAG'):\n",
        "            if 'gtzan-features.zip' in os.listdir('/content/gdrive/MyDrive'):\n",
        "                shutil.copy2('/content/gdrive/MyDrive/gtzan-features.zip', '.')\n",
        "            else:\n",
        "                # make sure to download the GTZAN dataset from \"https://drive.google.com/file/d/1LDlvwUeJ-h3JntNSmQABNV8z8oqltojO/view?usp=drive_link\"\n",
        "                # upload it to your own google drive for it to be copied in the previous if statement block\n",
        "                pass\n",
        "        else:\n",
        "            if pwd != '/content/dataset':\n",
        "                if 'gtzan-features.zip' in os.listdir(f'{pwd}/dataset'):\n",
        "                    shutil.copy2(f'{pwd}/dataset/gtzan-features.zip', '.')\n",
        "                else:\n",
        "                    raise Exception(\"Download the GTZAN dataset preprocessed.\")\n",
        "\n",
        "        with zipfile.ZipFile('gtzan-features.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "\n",
        "finally:\n",
        "    GENRES = os.listdir('/content/dataset/training/mfcc')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jht-abLDNqD"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDRogc3lDNqD"
      },
      "outputs": [],
      "source": [
        "def load_data(src: str, feature: str, \n",
        "              random_state: float = SEED_VALUE, shuffle: bool = True, \n",
        "              stratify: list = None):\n",
        "    dataset = []\n",
        "    for genre in os.listdir(f'{src}/{feature}'):\n",
        "        for img in os.listdir(f'{src}/{feature}/{genre}'):\n",
        "            img = cv2.imread(f'{src}/{feature}/{genre}/{img}')\n",
        "            img = cv2.resize(img, (256, 192))\n",
        "            img = np.array(img, dtype=np.float32)\n",
        "            dataset.append([img, genre])\n",
        "\n",
        "    df = DataFrame(data=np.array(dataset, dtype=object), columns=[feature, 'genre'])\n",
        "\n",
        "    one_hot = pd.get_dummies(df['genre'])\n",
        "\n",
        "    df = pd.concat([df, one_hot], axis=1)\n",
        "    df.drop(['genre'], axis=1, inplace=True)\n",
        "\n",
        "    return (np.array([tf.convert_to_tensor(img) for img in df['mfcc']]), df[GENRES])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfOsnG1BDNqE"
      },
      "outputs": [],
      "source": [
        "(X_train, _) = load_data('./training/', 'mfcc', stratify=GENRES)\n",
        "(X_test, _) = load_data('./tests/', 'mfcc', stratify=GENRES)\n",
        "(X_val, _) = load_data('./validation/', 'mfcc', stratify=GENRES)\n",
        "\n",
        "# for cross validation purposes it is encouraged to concatenate all input/output data, in this case, only input\n",
        "X_all = np.concatenate((X_train, X_test), axis=0)\n",
        "X_all = np.concatenate((X_all, X_val), axis=0)\n",
        "# this is just to normalize the values between 0 and 1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "X_val = X_val / 255\n",
        "X_all = X_all / 255"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bXuwBf8uDNqE"
      },
      "source": [
        "## CNN Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2KtEizDNqF"
      },
      "outputs": [],
      "source": [
        "def generate_autoencoder():\n",
        "    input_l=Input(shape=(192, 256, 3))\n",
        "\n",
        "    maxp_ini=MaxPooling2D((4,4), padding='same', strides=(2, 2))(input_l)\n",
        "    encoding_1=Conv2D(12, (3,3), activation='relu',padding='same')(maxp_ini)\n",
        "\n",
        "    maxp_1=MaxPooling2D((2,2), padding='same')(encoding_1)\n",
        "    encoding_2=Conv2D(6, (3,3), activation='relu',padding='same')(maxp_1)\n",
        "\n",
        "    maxp_2=MaxPooling2D((2,2), padding='same')(encoding_2)\n",
        "    encoding_3=Conv2D(3, (3,3), activation='relu',padding='same')(maxp_2)\n",
        "\n",
        "    flat_1=Flatten()(encoding_3)\n",
        "\n",
        "    bottleneck=Dense(500)(flat_1)\n",
        "\n",
        "    decoding_1=Dense(2304, activation='relu')(bottleneck)\n",
        "    resh_1=Reshape(target_shape=(24, 32, 3)) (decoding_1)\n",
        "\n",
        "    decoding_2=Conv2D(6, (3,3), activation='relu', padding='same')(resh_1)\n",
        "    Up_1=UpSampling2D((2,2))(decoding_2)\n",
        "\n",
        "    decoding_3=Conv2D(12, (3,3), activation='relu', padding='same')(Up_1)\n",
        "    Up_2=UpSampling2D((2,2))(decoding_3)\n",
        "\n",
        "    decoding_4=Conv2D(3, (3,3), activation='relu', padding='same')(Up_2)\n",
        "    output_l=UpSampling2D((2,2))(decoding_4)\n",
        "\n",
        "    autoencoder=Model(inputs=[input_l],outputs=[output_l])\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBeIyu3iDNqF",
        "outputId": "527a3fb3-54a4-4e71-eabe-c647ee1aa05e"
      },
      "outputs": [],
      "source": [
        "generate_autoencoder().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr4CfUOO7xgb"
      },
      "outputs": [],
      "source": [
        "model_storage = '/content'\n",
        "\n",
        "if os.getenv('COLAB_RELEASE_TAG'):\n",
        "    model_storage = '/content/gdrive/MyDrive'\n",
        "\n",
        "if 'model_storage_final' not in os.listdir(model_storage):\n",
        "    os.mkdir(f'{model_storage}/model_storage_final')\n",
        "\n",
        "model_storage='/content/gdrive/MyDrive/model_storage_final'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kSY-S-Z3ZCX"
      },
      "source": [
        "## Training and Cross Validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhu2oqEMDNqF",
        "outputId": "938c75c8-0892-44da-aaae-eea9b8a35266"
      },
      "outputs": [],
      "source": [
        "autoencoder = generate_autoencoder()\n",
        "last_iter = 0\n",
        "iter_num = 15\n",
        "\n",
        "if len([autoencoder for autoencoder in os.listdir(model_storage) if autoencoder[:19]=='autoencoder_model2_']) > 0:\n",
        "    last_iter = sorted([int(autoencoder.removeprefix(f'autoencoder_model2_').removesuffix('.keras'))\n",
        "        for autoencoder in os.listdir(model_storage) if autoencoder[:19]=='autoencoder_model2_'], reverse=True)[0]\n",
        "    autoencoder = tf.keras.saving.load_model(f'{model_storage}/autoencoder_model2_{last_iter}.keras')\n",
        "    last_iter+=1 #this is for not overwriting the last saved model\n",
        "\n",
        "for i in range(last_iter, iter_num):\n",
        "    autoencoder.fit(x=X_train, y=X_train, epochs=10, validation_data=(X_test, X_test))\n",
        "    autoencoder.save(f'{model_storage}/autoencoder_model2_{i}.keras')\n",
        "\n",
        "# Generate validation set metrics\n",
        "scores = autoencoder.evaluate(X_val, X_val, verbose=0)\n",
        "print(f'Score for validation set: {autoencoder.metrics_names[0]} of {scores[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfSYG4kI08z3"
      },
      "outputs": [],
      "source": [
        "kfold=KFold(n_splits=10, shuffle=True)\n",
        "kfold_splits=[]\n",
        "\n",
        "for train, test in kfold.split(X_all, X_all):\n",
        "    kfold_splits.append((train, test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "h41zLs_B1Bzu",
        "outputId": "d8d697b7-c001-4ef3-a19f-b3ffd19a027c"
      },
      "outputs": [],
      "source": [
        "fold_no=1\n",
        "final_errors_per_fold=[]\n",
        "\n",
        "for train, test in kfold_splits:\n",
        "    last_iter = 0\n",
        "\n",
        "    autoencoder = generate_autoencoder()\n",
        "\n",
        "    if f'fold_{fold_no}' not in os.listdir(model_storage):\n",
        "        os.mkdir(f'{model_storage}/fold_{fold_no}')\n",
        "\n",
        "    if len([autoencoder for autoencoder in os.listdir(f'{model_storage}/fold_{fold_no}') if autoencoder[:19]=='autoencoder_model2_']) > 0:\n",
        "        last_iter = sorted([int(autoencoder.removeprefix(f'autoencoder_model2_').removesuffix('.keras'))\n",
        "            for autoencoder in os.listdir(f'{model_storage}/fold_{fold_no}') if autoencoder[:19]=='autoencoder_model2_'], reverse=True)[0]\n",
        "        autoencoder = tf.keras.saving.load_model(f'{model_storage}/fold_{fold_no}/autoencoder_model2_{last_iter}.keras')\n",
        "        last_iter+=1 #this is for not overwriting the last saved model\n",
        "\n",
        "    for i in range(last_iter, iter_num):\n",
        "        autoencoder.fit(x=X_all[train], y=X_all[train], epochs=10)\n",
        "        autoencoder.save(f'{model_storage}/fold_{fold_no}/autoencoder_model2_{i}.keras')\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = autoencoder.evaluate(X_all[test], X_all[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {autoencoder.metrics_names[0]} of {scores[0]}')\n",
        "    final_errors_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "\n",
        "plt.plot(final_errors_per_fold, label='fold test set')\n",
        "plt.title('Loss Function (MSE)')\n",
        "plt.xlabel('Fold #')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kub5xu74yJal"
      },
      "source": [
        "## Overfitting analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "daS1XFnNyH8L",
        "outputId": "52a86fdd-40f5-4a97-9f78-a58f9b77fe8d"
      },
      "outputs": [],
      "source": [
        "autoencoder = generate_autoencoder()\n",
        "last_iter = 0\n",
        "iter_num = 50\n",
        "history_train=[]\n",
        "history_test=[]\n",
        "\n",
        "for i in range(last_iter, iter_num):\n",
        "    if f'overfitting_analysis_{i}.keras' in [autoencoder for autoencoder in os.listdir(model_storage)]:\n",
        "        autoencoder = tf.keras.saving.load_model(f'{model_storage}/overfitting_analysis_{i}.keras')\n",
        "    else:\n",
        "        autoencoder.fit(x=X_train, y=X_train, epochs=10, validation_data=(X_test, X_test))\n",
        "\n",
        "    scores_train = autoencoder.evaluate(X_train, X_train, verbose=0)\n",
        "    history_train.append(scores_train[0])\n",
        "\n",
        "    scores_test = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
        "    history_test.append(scores_test[0])\n",
        "\n",
        "    if f'overfitting_analysis_{i}.keras' not in [autoencoder for autoencoder in os.listdir(model_storage)]:\n",
        "        autoencoder.save(f'{model_storage}/overfitting_analysis_{i}.keras')\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history_train, label='train')\n",
        "plt.plot(history_test, label='test')\n",
        "plt.title('Loss Function (MSE)')\n",
        "plt.xlabel('Save')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LNqDoyxy1iMY"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emTGNskG2c_6",
        "outputId": "f087249c-99f0-447d-f3bf-f5619a413c1e"
      },
      "outputs": [],
      "source": [
        "def build_encoder(autoencoder_name):\n",
        "    if autoencoder_name not in os.listdir(model_storage):\n",
        "        print(f'Not finished optimizing: {autoencoder_name} not found in {model_storage}.')\n",
        "    else:\n",
        "        autoencoder = tf.keras.saving.load_model(f'{model_storage}/{autoencoder_name}')\n",
        "        encoder_input_l = Input(shape=(192,256,3))\n",
        "        res_1 = autoencoder.layers[1](encoder_input_l)\n",
        "        res_2 = autoencoder.layers[2](res_1)\n",
        "        res_3 = autoencoder.layers[3](res_2)\n",
        "        res_4 = autoencoder.layers[4](res_3)\n",
        "        res_5 = autoencoder.layers[5](res_4)\n",
        "        res_6 = autoencoder.layers[6](res_5)\n",
        "        res_7 = autoencoder.layers[7](res_6)\n",
        "        encoder_output_l = autoencoder.layers[8](res_7)\n",
        "        encoder=Model(inputs=[encoder_input_l],outputs=[encoder_output_l])\n",
        "        return encoder\n",
        "\n",
        "# if necessary, download this precise autoencoder model from https://drive.google.com/file/d/12gtgkBgo_n1SF6LKLzNsXgtTuN_Wcoqn/view?usp=sharing\n",
        "encoder = build_encoder('autoencoder_model2_14.keras')\n",
        "# encoder is already saved at https://drive.google.com/file/d/11odRe5Stk9x8BWTv5Y2x8TFvgT_iuuZs/view?usp=sharing\n",
        "encoder.save(f'{model_storage}/encoder_model.keras')\n",
        "\n",
        "print(encoder(X_train[:1])) # test"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
